Title: The Black Swan Concept
Category: philosophy/incerto
Tags: risk,sitg,knowledge

My attempt at [grokking]() NNT's book, _The Black Swan_. 

The Black Swan analagy

> illustrates a severe limitation to our learning from observations or experience and the fragility of out knowledge. One single observation can invalidate a general statement [all swans are white] derived from millennia of confirmatory sightings of millions of white. All you need is one single black bird...  it is about the oversize role of extreme events in many domains in life. 
>

1. an _outlier_, lying outside the realm of expectations, because nothing in the past can convincingly point to its possibility,
2. carries extreme impact, 
3, in spite of outlier status, human nature [hubris, man's search for meaning and making sense of the world] makes us concoct explanations for its occurrence after the fact, making it "explainable and predictable" -> retrospective predictability

> the central idea... concerns our blindness with respect to randomness, particularly the large deviations...
>
> It is easy to see that life is the cumulative effect of a handful of significant shocks... Look into your own existence. Count the significant events... how many of them came on a schedule [according to plan]?

> Black swan logic makes _what you don't know_ far more relevant that what you do know... The black swan is the result of epistemic limitations (distortions), mostly confidence in knowledge. 


> the payoff of a human venture is in general inversely proportional to what it is expected to be... The more unexpected the success of such a venture, the smaller number of competitors, and the more successful the entrepreneur who implements the idea.
>

> _the inability to predict outliers implies to inability to predict the course of history... but we act as though we are able to predict... Owing to this misunderstanding of the causal chains between policy and actions, we can easily trigger black swans thanks to _aggressive ignorance_, like a child playing with a chemistry kit. 

>Black swans being unpredictable, we need to adjust to their existence rather than naively try to predict them... Indeed, in some domains, such as scientific discovery and venture capital investments, there is a disproportionate payoff from the unknown, since you typically have little to lose and plenty to gain. 

>We will see that, contrary to social science wisdom, almost no discovery, no technologies of note, came from [foremost] from design and planning -- the were just Black Swans. The strategy for discoverers and entrepreneurs is to rely less on top-down planning and focus on maximum tinkering and recognizing opportunities as they present themselves... the reason free markets work is because they allow a few people to be lucky, thanks to aggressive trial and error, not by giving rewards or incentives for skills. The strategy is then, to tinker as much as possible and try to collect as many Black Swan opportunities as you can. 

> the story of the maginot line shows how we are conditioned to be specific... the French, after the Great War, built a wall along the previous German invasion route to prevent reinvasion- Hitler just went effortlessly around it. The French were excellent students of history; they just learned with too much precision... we do not spontaneously learn that we don't learn that we don't learn. The problem lies in the structure of our minds: we don't learn rules [guidelines], just facts. Metarules (such as the rule that we have a tendency to not learn rules) we don't seem to be good at getting.


Recursive concept, XXVII

> Everyone body knows that you need more prevention than treatment, but few reward acts of prevention. We glorify those who left the names in our history books at the expense of those contributors about whom our books are silent. We humans are not just a superficial race; we are a very unfair one. << OBSCURITY.

> Platonicity is our tendency to mistake the map for the territory, to focus on pure and well-defined "forms"... When these ideas and crisp constructs inhabit our minds, we privilege them over other less elegant objects, those with messier and less tractable structures. Platonicity is what makes us think that we understand more than we actually do... the Platonic Fold is the explosive boundary where the Platonic mindset enters in contact with messy reality, where the gap between what you know and what you think you know becomes dangerously wide. 

> Categorizing always produces reduction in true complexity... Any reduction f the world around us can have explosive consequences since it rules out sources of uncertainly; it drives us to a misunderstanding of the fabric of the world [reality].

> Humans, in the desire for intellectual comfort to, to narrate the world and make sense of events, to find meaning, try to explain and categorize, when, in reality, it's all far to complicated and chaotic for one to make sense of it all without excessive reductionism. This reduction and loss of knowledge and wisdom when we fit reality to our mental models often does more harm than good. 

> the beats is not just the bell curve or the self-deceiving statistician or the Platonified scholar... it is [man's] drive to "focus" on what makes sense to us. Living on the planet today requires a lot more imagination than we are made to have... Our world is dominated by the extreme, the unknown, and the very improbable (according to current knowledge), and all the while we focus on the known and the repeated... We need to use the extreme event as a starting point, and not treat it as an exception to be swept under the rug... in spite of our progress and growth in knowledge, or perhaps because of such progress and growth, the future will be increasingly less predictable, while both human nature and social science seem to conspire to hide the idea from us. 

> Anti-scholar: someone who focuses on the unread books [in his library], and makes an attempt not to treat his knowledge as a treasure, or even a possession or even a self-esteem enhancement device -- a skeptical empiricist [who doesn't take what he knows too seriously]... We tend to treat our knowledge as personal property to be protected and defended.
>
## History and the Triplet of Opacity

> History is opaque. You see what comes out, not the script that produces events, the generator of history. There is a fundamental incompleteness in your grasp of such events, since you do not see what's inside the box, how the mechanisms work. What I call the generator of historical events is different from the events themselves, much as the minds of the gods cannot be read just by witnessing their deeds. You are very likely to be fooled by their intentions.

>The human mind suffers from three ailments as it comes in contact with history, what I call the triplet of opacity:

1. The _illusion of understanding_, or how everyone thinks he know what is going on in a world that is more complicated (or random, distinctly) than they realize
2. the retrospective distortion (and plausibility), or how we can assess matters only after the fact; history seems clearer and more organized in history books than in empirical reality [history books storytell].
3. the overvaluaction of factual information [relative to what is _unknown_] and the handicap of authoritative and learned [as being overconfident in their base of facts]

> Our minds are wonderful explanation machines, capable of making sense out of almost anything, capable of mounting explanations for all matter of phenomena, and generally incapable of accepting the idea of unpredictability... the studious examination of the past in the greatest of detail does not teach you about the Mind of History [as a abstract being and orchestrator]; it only gives you the illusion of understanding it. History and societies do not crawl. They make umps. They go from fracture to fracture [piecewise] with a few vibrations in between. Yet we (and historians) like to believe in the predictable, small incremental [gradual, steady] progression. 

Berlin Diary: The Journal of a Foreign Correspondent, 1934-1941 pg 12.

## The Problem with the Media & Journalism: Clustering

> journalists tend[ed] to cluster not necessarily around the same opinions but frequently around the same framework of analysis. They assign the same importance to the same sets of circumstances and cut reality into the same categories [a manifestation of Platonicity]... Categorizing is [evolutionarily] necessary for humans, but it becomes pathological when the category is seen as definitive [written in stone], preventing people from considering the fuzziness of boundaries, let alone revising their categories.
>
> If you selected one hundred independent-minded journalists capable of seeing factors in isolation from one another, you would get one hundred different opinions. But the process of having these people report in lockstep caused the dimensionality of the opinion set to shrink considerably -- they converged on opinions and used the same items as caused. 

Skipped ch 3.

> The entire knowledge-seeking enterprise is based on taking conventional wisdom and accepted scientific beliefs and shattering them into pieces with new counterintuitive evidence... Scientists may be in the business of laughing at their predecessors, but owing to an array of human mental dispositions, few realize that someone will laugh at their beliefs in the future.

## Bertrand's Problem of Induction

> How do we know that what we have observed from objects and events suffices to enable us to figure out their other properties... how can we figure out properties of the infinite unknown based on the finite known?

> Consider a turkey that is fed every day. Every single feeding will firm up the bird's belief that it is the general rule of life to be fed every day by friendly members of the human race "looking out for its best interests" as a politician would say... this problem can be generalized to any situation where the same hand that feeds you can be the one that wrings your neck... Let's go one step further and consider induction's most worrisome aspect: learning backwards. 

> Consider that the turkey's experience may have, rather than no value, a negative value... Its confidence increased the number of friendly feedings grew, and it felt increasingly safe even though the slaughter was more and more imminent... the feeling of safety reached its maximum when the risk was at the highest!
 
> Making a naive observations of the past as something definitive of representative of the future is the one and only cause of our inability to understand the black swan... a Black swan is relative to knowledge... you can eliminate a black swan by keeping an open mind. 

> Erudition signals genuine intellectual curiosity. It accompanies an open mind and the desire to probe the ideas of others. Above all, an erudite can be dissatisfied with his own knowledge, and such dissatisfaction is a wonderful shield against Platonicity [and the [Dunning-Kruger effect](https://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect)], and the simplification of the five-minute manager, or the philistinism of the overspecialized scholar. 

> ... themes arising from our Blindness to the Black Swan [due to us _thinking_ we live in Mediocristan]:

1. We focus on preselected segments of the seen and generalize from it to the unseen: _the error of confirmation_
2. We fool ourselves with stories that cater to our Platonic thirst for distinct patterns: _the narrative fallacy_
3. What we see is not necessarily all that is there. History hides Black Swans from us and gives us the mistaken idea about the odds of events [and their predictability]: _the distortion of silent evidence_
4. We "tunnel": that is we focus on a few well-defined sources of uncertainty, on too specific a list of Black Swans (at the expense of others that do not easily come to mind).

> See the _round-trip fallacy_ in the unfairness of stereotypes: minorities in urban areas have suffered the same confusion: even if most criminals come from their ethnic subgroup, most of their ethnic subgroup are not criminals, but they suffer from discrimination by people who should know better... Our inferential machinery... is not made for a complicated environment... Consider that in a primitive environment there is no consequential difference between the statements _most killers are wild animals_ and _most wild animals are killers_. There is an error here, but it is almost inconsequential. Our statistical intuitions have not evolved for a habitat in which these subtleties can make a big difference. 

> The inability to automatically transfer knowledge and sophistication from one situation to another, or from theory to practice is a quite disturbing attribute of human nature. Let us call it the _domain specificity_ of our reactions... our mode of thinking, intuitions, depend on the context [domain] in which the matter is presented... the classroom is one domain; real life is another. We react to a piece of information not on its logical merit, but on the basis of which framework surrounds it, and how it registers with our social-emotional system... Knowledge, even when it is exact, does not often lead to appropriate actions because we tend to forget what we know, or forget how to process it properly if we do not pay attention. 

> In 1971, Amos Tversky and Daniel Kahneman plied professors of statistics with statistical questions not phrased as statistical questions.... Statisticians, it has been shown, tend to leave their brains in the classroom and engage in the most trivial inferential errors once they are let out into the streets. 

> Doctors in the midst of _scientific arrogance_ looks down at mothers' milk as something primitive as if it could be replicated by their laboratories - not realizing that mothers' milk might include useful components _elude their scientific understanding_: a simple confusion of _absence of evidence_ of the benefits of mothers' milk and the _evidence of absence_ (another case of Platonicity as it "does not make sense" to breast-feed when we could simply use bottles)... Many people paid the price for this naive inference... Medicine has caused plenty of damage throughout history, owing to this simple kind of inferential confusion... definitive, closed beliefs need to be avoided... [advocate] skeptical-empirical medicine [science] that avoid[s] theorizing.

> naive empiricism: we have a natural tendency to look for instances that confirm our story and our vision of the world - these instances are always easy to find... you take past instances that corroborate your theories and you treat them as evidence.

## NEGATIVE EMPIRICISM

> A series of corroborative facts is not necessarily evidence. Seeing white swans does not confirm the existence of black swans [it only reinforces the belief that all swans are white]. If I see a black swan I can certify that all swans are not white... If I see someone kill, then I can be practically certain that he is a criminal. If I don't see him kill, I cannot be certain that he is innocent. 

> We can get closer to the truth by _negative_ instances, not by verification. It is misleading to build a general rule from observed facts. Contrary to conventional wisdom, our body of knowledge does not increase from a series of confirmatory observations, like the turkey's. 

> This asymmetry is immensely practical... Sometimes a lot of data can be meaningless; at other times one single piece of information can be very meaningful... _you know what is wrong with a lot more confidence that you know what is right_.

> The person who is credited with the promotion of this idea of one-sided semi-skepticism is Karl Popper... Popper's far more powerful and original idea is the "open" society, one that relies on skepticism as the modus operandi, refusing and resisting definitive truths. He accused Plato of closing our minds.

> Confirmation bias: our natural tendency to look only for corroboration. This confirmation problem pervades our modern life, since most conflicts have at their root the following mental bias: when Arabs and Israelis watch news reports they see different stories in the same succession of events. Democrats and Republicans look at different parts of the same data and never converse to the same opinions. Once you mind is inhabited with a certain view of the world, you will tend only to consider instances proving you right. Paradoxically, the more [filtered] information you have, the more justified you will feel in your views... the ability to look at the world without the need to find signs that stroke one's ego: that's true self-confidence.


## NARRATIVE FALLACY

We like stories, like like to summarize, and we like to simplify, i.e. to reduce the dimensions of matters... The fallacy is associated with our vulnerability to over-interpretation and out predilection for compact stories over raw truths. It severely distorts our mental representation of the world... it addresses our limited ability to look at sequences of facts without weaving an explanation into them, or equivalently, forcing a logical link, an arrow of relationship, upon them. Explanations bind facts together. They make them all the more easily remembered; they help them make more sense. Where this propensity can go wrong is when it increases our impression of understanding... it is more generally a problem of information. While generally narrativity comes from our biological need to reduce dimensionality, robots would be prone to same process of reduction. Information wants to be reduced. 

In studying the problem of induction [in the prev chapter], we examined what could be inferred about the unseen, what lies outside our information set. [The narrative fallacy] looks at the seen, what lies within our information set, [and] the distortions in the act of processing it... Our propensity to impose meaning and concepts blocks our awareness of the details.

A higher concentration of dopamine appears to lower skepticism and result in greater vulnerability to pattern detection... [it] seems to increase such activity and lowers one's suspension of belief. The person becomes vulnerable to all manner of fads...

There is another, even deeper reason for our inclination to narrate, and it is not psychological [or biological]. It has to do with the effect of order on information storage and retrieval in any system, and it's worth explaining here because of what I consider the central problems of probability and information theory:

1. information is costly to obtain,

2. information is costly to store: The more orderly, less random, patterned and "narratized" a series of words or symbols, the easier it is to store that series in one's mind

3. information is costly to manipulate and retrieve... With so many brain cells (100B+)... difficulties probably do not arise from storage capacity, but may just be indexing problems. You conscious or working memory, is considerably smaller... Compression [of information] is vital to the performance of conscious work... By finding a pattern, the logic of a series, you no longer need to memorize at all. You just store the pattern... a pattern is obviously more compact than raw information. You looked into the book and found a rule [and remembered just it]. It is along these lines that the great probabilist Andrey Kolmogorov defined the degree of randomness, "Kolmogorov complexity."

We, members of the human variety of primates, have a hunger for rules because we need to reduce the dimensions of matters so they can get into our heads. Or, rather sadly, so we can squeeze them into our heads. The more random information is, the greater the dimensionality, and thus the more difficult to summarize. The more you summarize, the more order you put in, the less randomness. Hence the same condition that makes us simplify pushes us to think that the world is less random than it actually is. And the Black Swan is what we leave out in the simplification. 

Both the artistic and scientific enterprises are the product of our need to reduce dimensions and inflict some order on things. Think of the world around you, laden with trillions of details. Try to describe it and you will find yourself tempted to weave a thread into what you are saying. A novel, a story, a myth, or a tale, all have the same function: they spare us from the complexity of the world and shield us from its randomness. Myths impart order to the disorder of human perception and the perceived "chaos of human experience."

Platonicity affects us here once again... [the] desire for order applies to scientific pursuits - it us just that, unlike art, the (stated) purpose of science is to get to the truth, not to give you a feeling of organization or make you feel better. We tend to use knowledge as therapy. 

Our tendency to perceive - to impose - narrativity and causality are symptoms of the same disease - dimension reduction... [both have] a chronological dimension and lead to the perception of the flow of time [in a single direction]... but memory and the arrow of time can get mixed up. Narrativity can viciously affect the remembrance of past events as follows: we will tend to more easily remember those fact from our past that fit a narrative, while we tend to neglect others that do not appear to play a causal role in the narrative... this simple inability to remember not the true sequence of events but a reconstructed one will make history appear in hindsight to be far more explainable that it actually was - or is... 

In reality, memory is dynamic - not static - like a paper on which new texts (or new versions of the same text) will be continuously recorded, thanks to the power of posterior information... Memory is more of a self-serving dynamic revision machine: you remember the last time you remembered the event, and, without realizing it, change the story at every subsequent remembrance. So we pull memories along causative lines, revising them involuntarily and unconsciously. We continuously renarrate past events in the light of what appears to make what we think of as logical sense after these events occur... While we believe that memory is fixed, constant, and connected, all this is very far from the truth. What makes sense according to information obtained subsequently will be remembered more vividly. 

Beyond our perceptional distortions, there is a problem with logic itself...Consider that two people can hold incompatible beliefs based on the exact same data... the logician W.V. Quine showed that there exist families of logically consistent interpretations and theories that can match a given series of facts... mere absence of nonsense may not be sufficient to make something true. 


