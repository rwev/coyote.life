Title: The Black Swan Concept
Category: philosophy/incerto
Tags: risk,sitg,knowledge

My attempt at [grokking]() NNT's book, _The Black Swan_. 

The Black Swan analagy

> illustrates a severe limitation to our learning from observations or experience and the fragility of out knowledge. One single observation can invalidate a general statement [all swans are white] derived from millennia of confirmatory sightings of millions of white. All you need is one single black bird...  it is about the oversize role of extreme events in many domains in life. 
>

1. an _outlier_, lying outside the realm of expectations, because nothing in the past can convincingly point to its possibility,
2. carries extreme impact, 
3, in spite of outlier status, human nature [hubris, man's search for meaning and making sense of the world] makes us concoct explanations for its occurrence after the fact, making it "explainable and predictable" -> retrospective predictability

> the central idea... concerns our blindness with respect to randomness, particularly the large deviations...
>
> It is easy to see that life is the cumulative effect of a handful of significant shocks... Look into your own existence. Count the significant events... how many of them came on a schedule [according to plan]?

> Black swan logic makes _what you don't know_ far more relevant that what you do know... The black swan is the result of epistemic limitations (distortions), mostly confidence in knowledge. 


> the payoff of a human venture is in general inversely proportional to what it is expected to be... The more unexpected the success of such a venture, the smaller number of competitors, and the more successful the entrepreneur who implements the idea.
>

> _the inability to predict outliers implies to inability to predict the course of history... but we act as though we are able to predict... Owing to this misunderstanding of the causal chains between policy and actions, we can easily trigger black swans thanks to _aggressive ignorance_, like a child playing with a chemistry kit. 

>Black swans being unpredictable, we need to adjust to their existence rather than naively try to predict them... Indeed, in some domains, such as scientific discovery and venture capital investments, there is a disproportionate payoff from the unknown, since you typically have little to lose and plenty to gain. 

>We will see that, contrary to social science wisdom, almost no discovery, no technologies of note, came from [foremost] from design and planning -- the were just Black Swans. The strategy for discoverers and entrepreneurs is to rely less on top-down planning and focus on maximum tinkering and recognizing opportunities as they present themselves... the reason free markets work is because they allow a few people to be lucky, thanks to aggressive trial and error, not by giving rewards or incentives for skills. The strategy is then, to tinker as much as possible and try to collect as many Black Swan opportunities as you can. 

> the story of the maginot line shows how we are conditioned to be specific... the French, after the Great War, built a wall along the previous German invasion route to prevent reinvasion- Hitler just went effortlessly around it. The French were excellent students of history; they just learned with too much precision... we do not spontaneously learn that we don't learn that we don't learn. The problem lies in the structure of our minds: we don't learn rules [guidelines], just facts. Metarules (such as the rule that we have a tendency to not learn rules) we don't seem to be good at getting.


Recursive concept, XXVII

> Everyone body knows that you need more prevention than treatment, but few reward acts of prevention. We glorify those who left the names in our history books at the expense of those contributors about whom our books are silent. We humans are not just a superficial race; we are a very unfair one. << OBSCURITY.

> Platonicity is our tendency to mistake the map for the territory, to focus on pure and well-defined "forms"... When these ideas and crisp constructs inhabit our minds, we privilege them over other less elegant objects, those with messier and less tractable structures. Platonicity is what makes us think that we understand more than we actually do... the Platonic Fold is the explosive boundary where the Platonic mindset enters in contact with messy reality, where the gap between what you know and what you think you know becomes dangerously wide. 

> Categorizing always produces reduction in true complexity... Any reduction f the world around us can have explosive consequences since it rules out sources of uncertainly; it drives us to a misunderstanding of the fabric of the world [reality].

> Humans, in the desire for intellectual comfort to, to narrate the world and make sense of events, to find meaning, try to explain and categorize, when, in reality, it's all far to complicated and chaotic for one to make sense of it all without excessive reductionism. This reduction and loss of knowledge and wisdom when we fit reality to our mental models often does more harm than good. 

> the beats is not just the bell curve or the self-deceiving statistician or the Platonified scholar... it is [man's] drive to "focus" on what makes sense to us. Living on the planet today requires a lot more imagination than we are made to have... Our world is dominated by the extreme, the unknown, and the very improbable (according to current knowledge), and all the while we focus on the known and the repeated... We need to use the extreme event as a starting point, and not treat it as an exception to be swept under the rug... in spite of our progress and growth in knowledge, or perhaps because of such progress and growth, the future will be increasingly less predictable, while both human nature and social science seem to conspire to hide the idea from us. 

> Anti-scholar: someone who focuses on the unread books [in his library], and makes an attempt not to treat his knowledge as a treasure, or even a possession or even a self-esteem enhancement device -- a skeptical empiricist [who doesn't take what he knows too seriously]... We tend to treat our knowledge as personal property to be protected and defended.
>
## History and the Triplet of Opacity

> History is opaque. You see what comes out, not the script that produces events, the generator of history. There is a fundamental incompleteness in your grasp of such events, since you do not see what's inside the box, how the mechanisms work. What I call the generator of historical events is different from the events themselves, much as the minds of the gods cannot be read just by witnessing their deeds. You are very likely to be fooled by their intentions.

>The human mind suffers from three ailments as it comes in contact with history, what I call the triplet of opacity:

1. The _illusion of understanding_, or how everyone thinks he know what is going on in a world that is more complicated (or random, distinctly) than they realize
2. the retrospective distortion (and plausibility), or how we can assess matters only after the fact; history seems clearer and more organized in history books than in empirical reality [history books storytell].
3. the overvaluaction of factual information [relative to what is _unknown_] and the handicap of authoritative and learned [as being overconfident in their base of facts]

> Our minds are wonderful explanation machines, capable of making sense out of almost anything, capable of mounting explanations for all matter of phenomena, and generally incapable of accepting the idea of unpredictability... the studious examination of the past in the greatest of detail does not teach you about the Mind of History [as a abstract being and orchestrator]; it only gives you the illusion of understanding it. History and societies do not crawl. They make umps. They go from fracture to fracture [piecewise] with a few vibrations in between. Yet we (and historians) like to believe in the predictable, small incremental [gradual, steady] progression. 

Berlin Diary: The Journal of a Foreign Correspondent, 1934-1941 pg 12.

## The Problem with the Media & Journalism: Clustering

> journalists tend[ed] to cluster not necessarily around the same opinions but frequently around the same framework of analysis. They assign the same importance to the same sets of circumstances and cut reality into the same categories [a manifestation of Platonicity]... Categorizing is [evolutionarily] necessary for humans, but it becomes pathological when the category is seen as definitive [written in stone], preventing people from considering the fuzziness of boundaries, let alone revising their categories.
>
> If you selected one hundred independent-minded journalists capable of seeing factors in isolation from one another, you would get one hundred different opinions. But the process of having these people report in lockstep caused the dimensionality of the opinion set to shrink considerably -- they converged on opinions and used the same items as caused. 

Skipped ch 3.

> The entire knowledge-seeking enterprise is based on taking conventional wisdom and accepted scientific beliefs and shattering them into pieces with new counterintuitive evidence... Scientists may be in the business of laughing at their predecessors, but owing to an array of human mental dispositions, few realize that someone will laugh at their beliefs in the future.

## Bertrand's Problem of Induction

> How do we know that what we have observed from objects and events suffices to enable us to figure out their other properties... how can we figure out properties of the infinite unknown based on the finite known?

> Consider a turkey that is fed every day. Every single feeding will firm up the bird's belief that it is the general rule of life to be fed every day by friendly members of the human race "looking out for its best interests" as a politician would say... this problem can be generalized to any situation where the same hand that feeds you can be the one that wrings your neck... Let's go one step further and consider induction's most worrisome aspect: learning backwards. 

> Consider that the turkey's experience may have, rather than no value, a negative value... Its confidence increased the number of friendly feedings grew, and it felt increasingly safe even though the slaughter was more and more imminent... the feeling of safety reached its maximum when the risk was at the highest!
 
> Making a naive observations of the past as something definitive of representative of the future is the one and only cause of our inability to understand the black swan... a Black swan is relative to knowledge... you can eliminate a black swan by keeping an open mind. 

> Erudition signals genuine intellectual curiosity. It accompanies an open mind and the desire to probe the ideas of others. Above all, an erudite can be dissatisfied with his own knowledge, and such dissatisfaction is a wonderful shield against Platonicity [and the [Dunning-Kruger effect](https://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect)], and the simplification of the five-minute manager, or the philistinism of the overspecialized scholar. 

> ... themes arising from our Blindness to the Black Swan [due to us _thinking_ we live in Mediocristan]:

1. We focus on preselected segments of the seen and generalize from it to the unseen: _the error of confirmation_
2. We fool ourselves with stories that cater to our Platonic thirst for distinct patterns: _the narrative fallacy_
3. What we see is not necessarily all that is there. History hides Black Swans from us and gives us the mistaken idea about the odds of events [and their predictability]: _the distortion of silent evidence_
4. We "tunnel": that is we focus on a few well-defined sources of uncertainty, on too specific a list of Black Swans (at the expense of others that do not easily come to mind).

> See the _round-trip fallacy_ in the unfairness of stereotypes: minorities in urban areas have suffered the same confusion: even if most criminals come from their ethnic subgroup, most of their ethnic subgroup are not criminals, but they suffer from discrimination by people who should know better... Our inferential machinery... is not made for a complicated environment... Consider that in a primitive environment there is no consequential difference between the statements _most killers are wild animals_ and _most wild animals are killers_. There is an error here, but it is almost inconsequential. Our statistical intuitions have not evolved for a habitat in which these subtleties can make a big difference. 

> The inability to automatically transfer knowledge and sophistication from one situation to another, or from theory to practice is a quite disturbing attribute of human nature. Let us call it the _domain specificity_ of our reactions... our mode of thinking, intuitions, depend on the context [domain] in which the matter is presented... the classroom is one domain; real life is another. We react to a piece of information not on its logical merit, but on the basis of which framework surrounds it, and how it registers with our social-emotional system... Knowledge, even when it is exact, does not often lead to appropriate actions because we tend to forget what we know, or forget how to process it properly if we do not pay attention. 

> In 1971, Amos Tversky and Daniel Kahneman plied professors of statistics with statistical questions not phrased as statistical questions.... Statisticians, it has been shown, tend to leave their brains in the classroom and engage in the most trivial inferential errors once they are let out into the streets. 

> Doctors in the midst of _scientific arrogance_ looks down at mothers' milk as something primitive as if it could be replicated by their laboratories - not realizing that mothers' milk might include useful components _elude their scientific understanding_: a simple confusion of _absence of evidence_ of the benefits of mothers' milk and the _evidence of absence_ (another case of Platonicity as it "does not make sense" to breast-feed when we could simply use bottles)... Many people paid the price for this naive inference... Medicine has caused plenty of damage throughout history, owing to this simple kind of inferential confusion... definitive, closed beliefs need to be avoided... [advocate] skeptical-empirical medicine [science] that avoid[s] theorizing.

> naive empiricism: we have a natural tendency to look for instances that confirm our story and our vision of the world - these instances are always easy to find... you take past instances that corroborate your theories and you treat them as evidence.

## NEGATIVE EMPIRICISM

> A series of corroborative facts is not necessarily evidence. Seeing white swans does not confirm the existence of black swans [it only reinforces the belief that all swans are white]. If I see a black swan I can certify that all swans are not white... If I see someone kill, then I can be practically certain that he is a criminal. If I don't see him kill, I cannot be certain that he is innocent. 

> We can get closer to the truth by _negative_ instances, not by verification. It is misleading to build a general rule from observed facts. Contrary to conventional wisdom, our body of knowledge does not increase from a series of confirmatory observations, like the turkey's. 

> This asymmetry is immensely practical... Sometimes a lot of data can be meaningless; at other times one single piece of information can be very meaningful... _you know what is wrong with a lot more confidence that you know what is right_.

> The person who is credited with the promotion of this idea of one-sided semi-skepticism is Karl Popper... Popper's far more powerful and original idea is the "open" society, one that relies on skepticism as the modus operandi, refusing and resisting definitive truths. He accused Plato of closing our minds.

> Confirmation bias: our natural tendency to look only for corroboration. This confirmation problem pervades our modern life, since most conflicts have at their root the following mental bias: when Arabs and Israelis watch news reports they see different stories in the same succession of events. Democrats and Republicans look at different parts of the same data and never converse to the same opinions. Once you mind is inhabited with a certain view of the world, you will tend only to consider instances proving you right. Paradoxically, the more [filtered] information you have, the more justified you will feel in your views... the ability to look at the world without the need to find signs that stroke one's ego: that's true self-confidence.


## NARRATIVE FALLACY

We like stories, like like to summarize, and we like to simplify, i.e. to reduce the dimensions of matters... The fallacy is associated with our vulnerability to over-interpretation and out predilection for compact stories over raw truths. It severely distorts our mental representation of the world... it addresses our limited ability to look at sequences of facts without weaving an explanation into them, or equivalently, forcing a logical link, an arrow of relationship, upon them. Explanations bind facts together. They make them all the more easily remembered; they help them make more sense. Where this propensity can go wrong is when it increases our impression of understanding... it is more generally a problem of information. While generally narrativity comes from our biological need to reduce dimensionality, robots would be prone to same process of reduction. Information wants to be reduced. 

In studying the problem of induction [in the prev chapter], we examined what could be inferred about the unseen, what lies outside our information set. [The narrative fallacy] looks at the seen, what lies within our information set, [and] the distortions in the act of processing it... Our propensity to impose meaning and concepts blocks our awareness of the details.

A higher concentration of dopamine appears to lower skepticism and result in greater vulnerability to pattern detection... [it] seems to increase such activity and lowers one's suspension of belief. The person becomes vulnerable to all manner of fads...

There is another, even deeper reason for our inclination to narrate, and it is not psychological [or biological]. It has to do with the effect of order on information storage and retrieval in any system, and it's worth explaining here because of what I consider the central problems of probability and information theory:

1. information is costly to obtain,

2. information is costly to store: The more orderly, less random, patterned and "narratized" a series of words or symbols, the easier it is to store that series in one's mind

3. information is costly to manipulate and retrieve... With so many brain cells (100B+)... difficulties probably do not arise from storage capacity, but may just be indexing problems. You conscious or working memory, is considerably smaller... Compression [of information] is vital to the performance of conscious work... By finding a pattern, the logic of a series, you no longer need to memorize at all. You just store the pattern... a pattern is obviously more compact than raw information. You looked into the book and found a rule [and remembered just it]. It is along these lines that the great probabilist Andrey Kolmogorov defined the degree of randomness, "Kolmogorov complexity."

We, members of the human variety of primates, have a hunger for rules because we need to reduce the dimensions of matters so they can get into our heads. Or, rather sadly, so we can squeeze them into our heads. The more random information is, the greater the dimensionality, and thus the more difficult to summarize. The more you summarize, the more order you put in, the less randomness. Hence the same condition that makes us simplify pushes us to think that the world is less random than it actually is. And the Black Swan is what we leave out in the simplification. 

Both the artistic and scientific enterprises are the product of our need to reduce dimensions and inflict some order on things. Think of the world around you, laden with trillions of details. Try to describe it and you will find yourself tempted to weave a thread into what you are saying. A novel, a story, a myth, or a tale, all have the same function: they spare us from the complexity of the world and shield us from its randomness. Myths impart order to the disorder of human perception and the perceived "chaos of human experience."

Platonicity affects us here once again... [the] desire for order applies to scientific pursuits - it us just that, unlike art, the (stated) purpose of science is to get to the truth, not to give you a feeling of organization or make you feel better. We tend to use knowledge as therapy. 

Our tendency to perceive - to impose - narrativity and causality are symptoms of the same disease - dimension reduction... [both have] a chronological dimension and lead to the perception of the flow of time [in a single direction]... but memory and the arrow of time can get mixed up. Narrativity can viciously affect the remembrance of past events as follows: we will tend to more easily remember those fact from our past that fit a narrative, while we tend to neglect others that do not appear to play a causal role in the narrative... this simple inability to remember not the true sequence of events but a reconstructed one will make history appear in hindsight to be far more explainable that it actually was - or is... 

In reality, memory is dynamic - not static - like a paper on which new texts (or new versions of the same text) will be continuously recorded, thanks to the power of posterior information... Memory is more of a self-serving dynamic revision machine: you remember the last time you remembered the event, and, without realizing it, change the story at every subsequent remembrance. So we pull memories along causative lines, revising them involuntarily and unconsciously. We continuously renarrate past events in the light of what appears to make what we think of as logical sense after these events occur... While we believe that memory is fixed, constant, and connected, all this is very far from the truth. What makes sense according to information obtained subsequently will be remembered more vividly. 

Beyond our perceptional distortions, there is a problem with logic itself...Consider that two people can hold incompatible beliefs based on the exact same data... the logician W.V. Quine showed that there exist families of logically consistent interpretations and theories that can match a given series of facts... mere absence of nonsense may not be sufficient to make something true. 

We harbor a crippling dislike for the abstract [unknown]... Whenever there is a market move, the news media feel obligated to give the "reason"... It happens all the time: a cause is proposed to make you swallow the news and make matters more concrete... the problem of overcausation does not lie with the journalist, but with the public.... We want to be told stories, and there is nothing wrong with that - except that we should check more thoroughly whether the story provides consequential distortions of reality. 

Just consider that the newspaper try to get impeccable facts, but weave them into a narrative in such a way as to convey the impression of causality (and knowledge). There are fact-checkers, not intellect-checkers.

General, we like to think about specific and known Black Swans when in the fact the very nature of randomness lies in its abstraction ("the unknown unknowns").

# THE SHORTCUTS

System 1, the experiential one, is effortless, automatic fast, opaque (we do not know that we are using it), parallel-processed, "intuition", and performs quick acts of prowess [popularized for Malcolm Gladwell's _Blink_]... highly emotional, it produces shortcuts, called "heuristics and biases", that allow us to function rapidly and effectively "fast and frugal" "quick and dirty."

System 2, the cogitative, called "thinking", effortful, reasoned, slow, logical, serial, progressive, self-aware (you can follow steps in your own reasoning), tracable, retrospectively adaptable

You have to make an effort (System 2) to override your first reaction. Clearly Mother Nature makes you use the fast System 1 to get out of trouble, so that you do no sit down and cogitate whether there is truly a wild animal attacking you or if it is an optical illusion. You run immediately before you become "conscious" of the presence of the tiger. 

Emotions are assumed to be the weapon System 1 uses to direct us and force us to act quickly. It mediates risk avoidance far more effectively than our cognitive system. 

Much of the trouble with human nature resides in our inability to use much of system 2. 

neurobiologists make a similar distinction to that between system 1 and system 2, except that they operate along anatomical lines [instead of psychological and empirical], the cortical part, which we we are supposed to use for thinking, and which distinguishes us from other animals, and the fast-reacting limbic brain, which is the center of emotions, and which we share with other mammals. 

Our misunderstanding of the Black Swan can be largely attributed to our using System 1, i.e. narratives, and the sensational [mental availability bias] - as well as the emotional - which imposes on us a wrong map of the likelihood of events.


To avoid the ills of the narrative fallacy... favor experimentation over storytelling, experience over history, and clinical knowledge over theories... Being empirical doesn't mean running a laboratory in one's basement: it is just a mind-set that favors a certain class of knowledge over others. 

## CONCENTRATION OF SUCCESS 

Intellectual, scientific, and artistic activities belong to Extremistan, where there is a severe concentration of success, with a very small number of winners claiming a large share of the pot... Acknowleding the role of this concentration of success, and acting accordingly, causes us to be punished twice: we live in a society where the reward mechanism is based on the illusion of the regular; our hormonal reward system is also needs tangible and steady results. It too thinks the world is well behaved- it falls for the confirmation error. The world has changed too fast for our genetic makeup. We are alienated from our environment. 

Finding nothing is very valuable, since is it part of the process of discovery. 

Our intuitions are not cut out for nonlinearities. Consider our life in a primitive environment where process and success are closely connected... more work will lead to more apparent results, so your mood is propped up by visible continuous feedback... Our emotional apparatus is designed for linear causality... If you feel that you are not going anywhere, your emotions will cause you to become demoralized. But modern reality rarely gives us the privilege of a satisfying, linear, positive progression... the logical part of our mind, that "higher one" [cortical System 2], which distinguishes us from animals, can override our animal instinct, which asks for immediate rewards. 

Mother Nature destined us to derive enjoyment from a steady flow of pleasant small, but frequent rewards... our major satisfaction for there of years came in the form of food and water (and something else more private)... the problem is that we do not live in an environment where results are delivered in a steady manner... it is unfortunate that the right strategy for our current environment may not offer internal rewards and positive feedback.

Nonlinear relationships are ubiquitous in life. Linear relationships are truly the exception; we only focus on them in classrooms and textbooks because they are easier to understand... with linearity, relationships between variables are clear, crisp, and constant, therefore Platonically easy to grasp in a single sentence...

# The Problem of Silent Evidence

Roman stoic Cicero presented the following story. One Diagoras, a nonbeliever in the gods, was shown painted tablets bearing the portraits of some worshippers who prayed, then survived a subsequent shipwreck. The implication was that praying protect one from drowning. Diagoras asked, "where were the pictures of those who prayed, then drowned?"... The drowned worshippers, being dead, would have a lot of trouble advertising their experiences from the bottom of the sea. This can fool the casual observer into believing in miracles. 

AS drowned worshippers do not write histories of their experiences (it is better to be alive for that) so it is with the losers of history, whether people or ideas. 

"Such is the way of all superstition, whether in astrology, dreams, omens, divine judgements, or the like" - Francis Bacon

Silent evidence pervades everything connected to the notion of history. (History being "any succession of events seen with the effect of posteriority")... unless they are drilled into us systematically, or integrated into our way of thinking, these great observations are rapidly forgotten. 

This bias extends to
- the ascription of factors in the success of ideas and religions,
- the illusion of skill in many professions,
- success in artistic occupations,
- nature vs nurture debate,
- mistakes in using evidence in the court of law,
- illusions about the "logic" of history,
- most severely (relevant to Black Swan), the nature of extreme events.

It is so easy to avoid looking at the cemetery while concocting historical theories... it is a problem with the way we construct samples and gather evidence in every domain. 

The neglect of silent evidence is endemic to the way we study comparative talent, particular in activities plagued with winner-take-all attributes... there is no point reading too much into success stories because we do not see the full picture. 

Numerous studies of millionaire aimed at figuring out the skills required for hotshot-ness follow the following methodology. They take a population of hotshots, those with big titles and big jobs, and study their attributes. They look at what those big guns have in common: courage, risk-taking, optimism, and so on, and infer these traits help you become successful... Now consider the cemetery. The graveyard of failed persons will be full of people who shared the [same] traits.... what truly separates the two is luck.

## In government

"What We See and What We Don't See" Bastiat

We can see what governments do... but we do not see the alternative... governments are great at telling you what they did, but not what they did not do.... they engage in what could be labelled as phony philanthropy, the activity of helping people in a visible and sensational way without taking into account the unseen cemetery of invisible consequences... Consider job protection measures: you notice those jobs who are made safe and ascribe social benefits to such protections. You do _not_ notice the effect of those who cannot find a job as a result, since the measure will reduce job openings. 

We are explanation-seeking animals who tend to think everything has an identifiable cause and grab the most apparent as _the_ explanation. Yet there may not be a visible because; to the contrary, frequently there is nothing, not even a spectrum of possible explanations. But silent evidence masks this fact. Whenever our survival is in play, the very notion of because is severely weakened.

Silent evidence is not present in ["optimal" evolutionary fitness] theories. Evolution is a series of flukes, some good, many bad. You only see the good. But in the short term, it's not obvious which traits are really good for you, particularly if you are in [a] Black Swan-generating environment. 

Whenever survival is in play, don't immediately look for causes and effects. The main identifiable reason for [our] survival might simply be inaccessible to us: [we] are here since the "rosy" scenario played out... we are too brainwashed by notions of causality and we think that it is smarter to say _because_ than to accept randomness.

We are taking a condition, survival, and looking for the explanations, instead of flipping the argument on its head and stating _conditional on such survival_, one cannot read _that_ much into the process, and should learn to instead invoke some measure of randomness [equivalently pleading ignorance, as randomness, in practice, is what we don't know]... have the integrity to deliver your "because" very sparingly; try to limit it to situations where the "because" is derived from experiments, not backward-looking history. 

My biggest problem with the education system lies precisely in that it forces students to squeeze explanations out of  subject matters and _shames_ them for withholding judgement, for uttering the "I don't know."

I am not saying causes do not exist; do not use this argument to avoid trying to learn from history. All I am saying is that it is _not so simple_; be suspicious of the "because" and handle it with care -- particularly in situations where you suspect silent evidence... silent evidence [causes] deformation in our perception of empirical reality, making it appear more explainable (and more stable) than it actually is... our perceptual system may not react to what does not lie in front of our eyes, or what does not arouse our emotional attention. We are made to be superficial, to heed what we see and not heed what does not vividly come to mind... out of sight, out of mind: we harbor a natural, even physical, scorn of the abstract.


